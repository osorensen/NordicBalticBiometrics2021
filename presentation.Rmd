---
title: "Longitudinal modeling of age-dependent latent traits with generalized additive latent and mixed models"
author: "Øystein Sørensen"
institute: "Center for Lifespan Changes in Brain and Cognition</br>University of Oslo"
date: "Nordic-Baltic Biometrics Virtual Conference, 2021/06/08"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: inverse, middle, center

# Motivation

---

# Latent Variable Models

In psychology and social sciences, variables of interest are often measured with multiple *items*.

For example

- In psychometrics, latent abilities like working memory or executive function are defined by the response to multiple tasks.

- Surveys often use multiple questions to define a respondent's attitudes towards a given topic.

Classical tools for analyzing such data include factor analysis, item response theory, and structural equation models.

---

class: inverse, middle, center

# Motivating Example

---

# Episodic Memory

California verbal learning test: 

  - 16 words read out loud. Participant asked to repeat back.
  

![](figures/cvlt_sample_plot.png)

---

# Episodic Memory

- Ability to recall words depends nonlinearly on age.

- Ceiling effects. Later trials are easier.

- Repeated measurements. Five trials completed at each timepoint.

![](figures/cvlt_sample_plot.png)


---

# Episodic Memory

Research question:

  - How does episodic memory vary across the lifespan? Where is the peak, maximum increase, maximum decline?
  
  - How does the lifespan trajectory interact with variables like genetics, brain structure, socioeconomic status?

![](figures/cvlt_sample_plot.png)


---

class: inverse, middle, center

# Generalized Additive Latent and Mixed Models


---

# General Framework

- A combination of generalized additive mixed models<sup>1</sup> and generalized linear latent and mixed models<sup>2</sup>.

- Multilevel models with components varying at $L$ levels. $\boldsymbol{\eta}^{(l)}$ denotes a vector of latent variables varying at level $l$, and the vector of all latent variables belonging to a given level-2 unit is

$$\boldsymbol{\eta}_{j} = 
\begin{bmatrix}
\boldsymbol{\eta}_{jk\dots z}^{(2)}{}^{'},
\boldsymbol{\eta}_{k\dots z}^{(3)}{}^{'},
\dots,
\boldsymbol{\eta}_{z}^{(L)}{}^{'}
\end{bmatrix}'$$

- More details in the paper<sup>3</sup>.


.footnote[[1] For an introduction, see [Wood (2017), Generalized Additive Models](https://www.taylorfrancis.com/books/mono/10.1201/9781315370279/generalized-additive-models-simon-wood).<br>
[2] [Skrondal and Rabe-Hesketh (2004), Generalized Latent Variable Modeling](https://www.taylorfrancis.com/books/mono/10.1201/9780203489437/generalized-latent-variable-modeling-anders-skrondal-sophia-rabe-hesketh).<br>
[3] [Sørensen, Fjell, and Walhovd (2021), arXiv preprint](https://arxiv.org/abs/2105.02488)
.]

---

# Part 1: Response Distribution

Response distributed according to exponential family.

$$f\left(y_{i} | \theta_{i}, \phi\right) = \exp\left\{\frac{y_{i}\theta_{i} - b\left(\theta_{i}\right)}{\phi} + c\left(y_{i}, \phi\right)  \right\}$$


Elementary units of observation indexed by $i$, e.g., a single trial in the California verbal learning test shown previously.

Link function may vary with index $i$.


---

# Part 2: Measurement Model

Linking the observed responses to latent variables $\eta_{m}^{(l)}$ of interest.

$$\nu_{i} = \sum_{s=1}^{S} f_{s}\left(\mathbf{x}_{i}\right) + \sum_{l=2}^{L}\sum_{m=1}^{M_{l}} \eta_{m}^{(l)} \mathbf{z}^{(l)}_{mi}{}^{'}\boldsymbol{\lambda}_{m}^{(l)}$$

Hierarchical model with $L$ levels. Each level $l$ has $M_{l}$ latent variables. Factor loadings $\boldsymbol{\lambda}_{m}^{(l)}$ link the latent variables to the observed measurements.

Smooth functions $f_{s}(\cdot)$, e.g., 

$$f_{s}\left(\mathbf{x}_{i}\right) = \sum_{k=1}^{B_{s}} \omega_{ks} b_{ks}\left(\mathbf{x}_{i}\right), ~ s=1, \dots, S.$$

---

# Part 3: Structural Model

Structural model

$$\boldsymbol{\eta}_{j} = \mathbf{B}\boldsymbol{\eta}_{j} + \mathbf{h}\left(\mathbf{w}_{j}\right)+ \boldsymbol{\zeta}_{j}$$

$\mathbf{B}$ contains regression coefficients between latent variables.

$\mathbf{h}\left(\mathbf{w}_{j}\right)$ are smooth functions describing how the latent variables depend on explanatory variables.

$$\mathbf{h}\left(\mathbf{w}_{j}\right) = 
\begin{bmatrix}
\mathbf{h}_{2}\left\{\mathbf{w}_{j}^{(2+)}\right\} \\
\mathbf{h}_{3}\left\{\mathbf{w}_{j}^{(3+)}\right\} \\
\vdots \\
\mathbf{h}_{L}\left\{\mathbf{w}_{j}^{(L)}\right\} 
\end{bmatrix};
~
\mathbf{h}_{l}\left\{\mathbf{w}_{j}^{(l+)}\right\} = 
\begin{bmatrix}
h_{l1}\left\{\mathbf{w}_{j}^{(l+)}\right\} \\
h_{l2}\left\{\mathbf{w}_{j}^{(l+)}\right\} \\
\vdots \\
h_{l,M_{l}}\left\{\mathbf{w}_{j}^{(l+)}\right\}
\end{bmatrix}$$


---

# What's new?

- The functions $f_{s}\left(\mathbf{x}_{i}\right)$ in the measurement model and $\mathbf{h}\left(\mathbf{w}_{j}\right)$ are completely arbitrary. We typically assume that they are linear combinations of certain basis functions, subject to a smoothing penalty. 

- Allows flexible nonlinear modeling without requiring the parametric form to be a priori specified.

- Some Bayesian developments along these lines have been proposed, but all restricted to only two levels. With this framework, an arbitrary number of levels is allowed, as well as crossed random effects.


---

# Mixed Model Representation

- Assume the smooth functions are subject penalization. 

- For example, second derivative penalization amounts to adding terms of the form $\gamma^{f}_{s} \int f_{s}''(\mathbf{u})^{2}\text{d}\mathbf{u}$ and $\gamma^{h}_{lm} \int h_{lm}''(\mathbf{u})^{2}\text{d}\mathbf{u}$ to the log-likelihood.

- This type of penalized spline problem can be represented as a mixed model<sup>1</sup>. The part of the smooth functions in the penalty nullspace are fixed effects, and the part in the penalty range space are random effects.

- The proposed generalized additive latent and mixed models can be shown to be equivalent to nonlinear mixed models, with a number of parameter constraints.

.footnote[[1] Dating back to [Kimeldorf and Wahba (1970)](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-41/issue-2/A-Correspondence-Between-Bayesian-Estimation-on-Stochastic-Processes-and-Smoothing/10.1214/aoms/1177697089.full).]


---

# Profile Likelihood Estimation

---


class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](https://yihui.org/knitr/), and [R Markdown](https://rmarkdown.rstudio.com).
